{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": "from scipy.stats import truncnorm\n\n\ndef truncated_normal(mean=0, sd=1, low=0, upp=10):\n    return truncnorm(\n        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\n\n@np.vectorize\ndef sigmoid(x):\n    return 1 / (1 + np.e ** -x)\nactivation_function = sigmoid\n\nfrom scipy.stats import truncnorm\n\ndef truncated_normal(mean=0, sd=1, low=0, upp=10):\n    return truncnorm(\n        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n\n\nclass NeuralNetwork:\n        \n    \n    def __init__(self, \n                 no_of_in_nodes, \n                 no_of_out_nodes, \n                 no_of_hidden_nodes,\n                 learning_rate,\n                 bias=None\n                ):  \n\n        self.no_of_in_nodes = no_of_in_nodes\n        self.no_of_out_nodes = no_of_out_nodes\n        \n        self.no_of_hidden_nodes = no_of_hidden_nodes\n            \n        self.learning_rate = learning_rate \n        self.bias = bias\n        self.create_weight_matrices()\n    \n        \n    \n    def create_weight_matrices(self):\n        \"\"\" A method to initialize the weight matrices of the neural \n        network with optional bias nodes\"\"\"\n        \n        bias_node = 1 if self.bias else 0\n        \n        rad = 1 / np.sqrt(self.no_of_in_nodes + bias_node)\n        X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n        self.weights_in_hidden = X.rvs((self.no_of_hidden_nodes, \n                                       self.no_of_in_nodes + bias_node))\n\n        rad = 1 / np.sqrt(self.no_of_hidden_nodes + bias_node)\n        X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n        self.weights_hidden_out = X.rvs((self.no_of_out_nodes, \n                                        self.no_of_hidden_nodes + bias_node))\n        \n        \n        \n    def train(self, input_vector, target_vector):\n        # input_vector and target_vector can be tuple, list or ndarray\n        \n        bias_node = 1 if self.bias else 0\n        if self.bias:\n            # adding bias node to the end of the inpuy_vector\n            input_vector = np.concatenate( (input_vector, [self.bias]) )\n                                    \n            \n        input_vector = np.array(input_vector, ndmin=2).T\n        target_vector = np.array(target_vector, ndmin=2).T\n\n        \n        output_vector1 = np.dot(self.weights_in_hidden, input_vector)\n        output_vector_hidden = activation_function(output_vector1)\n        \n        if self.bias:\n            output_vector_hidden = np.concatenate( (output_vector_hidden, [[self.bias]]) )\n        \n        \n        output_vector2 = np.dot(self.weights_hidden_out, output_vector_hidden)\n        output_vector_network = activation_function(output_vector2)\n        \n        output_errors = target_vector - output_vector_network\n        # update the weights:\n        tmp = output_errors * output_vector_network * (1.0 - output_vector_network)     \n        tmp = self.learning_rate  * np.dot(tmp, output_vector_hidden.T)\n        self.weights_hidden_out += tmp\n\n\n        # calculate hidden errors:\n        hidden_errors = np.dot(self.weights_hidden_out.T, output_errors)\n        # update the weights:\n        tmp = hidden_errors * output_vector_hidden * (1.0 - output_vector_hidden)\n        if self.bias:\n            x = np.dot(tmp, input_vector.T)[:-1,:]     # ???? last element cut off, ???\n        else:\n            x = np.dot(tmp, input_vector.T)\n        self.weights_in_hidden += self.learning_rate * x\n        \n       \n    \n    def run(self, input_vector):\n        # input_vector can be tuple, list or ndarray\n        \n        if self.bias:\n            # adding bias node to the end of the inpuy_vector\n            input_vector = np.concatenate( (input_vector, [1]) )\n        input_vector = np.array(input_vector, ndmin=2).T\n\n        output_vector = np.dot(self.weights_in_hidden, input_vector)\n        output_vector = activation_function(output_vector)\n        \n        if self.bias:\n            output_vector = np.concatenate( (output_vector, [[1]]) )\n            \n\n        output_vector = np.dot(self.weights_hidden_out, output_vector)\n        output_vector = activation_function(output_vector)\n    \n        return output_vector\n            \n    "
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[(-3.2, -4.8), [0, 1]], [(4, 6), [1, 0]], [(3, 4), [1, 0]], [(-1.5, -3.6), [0, 1]], [(-3, -4), [0, 1]], [(3.7, 5.8), [1, 0]], [(4.2, 5.3), [1, 0]], [(6, 5), [1, 0]], [(5.2, 5.9), [1, 0]], [(4.3, 4.3), [1, 0]]]\n"
    }
   ],
   "source": "class1 = [(3, 4), (4.2, 5.3), (4, 3), (6, 5), (4, 6), (3.7, 5.8),\n          (3.2, 4.6), (5.2, 5.9), (5, 4), (7, 4), (3, 7), (4.3, 4.3) ] \nclass2 = [(-3, -4), (-2, -3.5), (-1, -6), (-3, -4.3), (-4, -5.6), \n          (-3.2, -4.8), (-2.3, -4.3), (-2.7, -2.6), (-1.5, -3.6), \n          (-3.6, -5.6), (-4.5, -4.6), (-3.7, -5.8) ]\n\nlabeled_data = []\nfor el in class1:\n    labeled_data.append( [el, [1, 0]])\nfor el in class2:\n    labeled_data.append([el, [0, 1]])\n  \n\nnp.random.shuffle(labeled_data)\nprint(labeled_data[:10])\n\ndata, labels = zip(*labeled_data)\nlabels = np.array(labels)\ndata = np.array(data)"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[0 1]\n[[0.06789332]\n [0.93747653]]\n[1 0]\n[[0.90325703]\n [0.09154966]]\n[1 0]\n[[0.89896926]\n [0.09624797]]\n[0 1]\n[[0.07723535]\n [0.9278203 ]]\n[0 1]\n[[0.06955122]\n [0.93570847]]\n[1 0]\n[[0.90294282]\n [0.09188639]]\n[1 0]\n[[0.9029385 ]\n [0.09192483]]\n[1 0]\n[[0.90359751]\n [0.09125664]]\n[1 0]\n[[0.90372345]\n [0.09107265]]\n[1 0]\n[[0.90191725]\n [0.09310423]]\n[0 1]\n[[0.06891982]\n [0.93638603]]\n[1 0]\n[[0.90210695]\n [0.09294748]]\n[0 1]\n[[0.07041998]\n [0.93485709]]\n[1 0]\n[[0.90322969]\n [0.09173542]]\n[0 1]\n[[0.07723905]\n [0.927655  ]]\n[0 1]\n[[0.06688572]\n [0.93854803]]\n[1 0]\n[[0.90093907]\n [0.09408948]]\n[0 1]\n[[0.0704938 ]\n [0.93483146]]\n[0 1]\n[[0.06672386]\n [0.9387201 ]]\n[1 0]\n[[0.90312767]\n [0.09164626]]\n[0 1]\n[[0.07472445]\n [0.93037842]]\n[0 1]\n[[0.06667012]\n [0.93876578]]\n[1 0]\n[[0.89747697]\n [0.09806028]]\n[0 1]\n[[0.06703487]\n [0.93833404]]\n"
    }
   ],
   "source": "simple_network = NeuralNetwork(no_of_in_nodes=2, \n                               no_of_out_nodes=2, \n                               no_of_hidden_nodes=10,\n                               learning_rate=0.1,\n                               bias=None)\n    \nfor _ in range(20):\n    for i in range(len(data)):\n        simple_network.train(data[i], labels[i])\nfor i in range(len(data)):\n    print(labels[i])\n    print(simple_network.run(data[i]))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
